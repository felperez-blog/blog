---
layout: post
title: Ai knowledge
subtitle:
cover-img:
thumbnail-img: /assets/img/ai.png
share-img:
tags: [ai, knowledge]
---


I find it kind of incredible it is when it comes to artifacts like Chatgpt, or Bard, or any of the LLMs. Ultimately it’s just a bunch of matrices. How can just a bunch of numbers basically fit a great amount of knowledge? It’s just incredible to me. I suppose that the architecture also holds a lot of information too, at a higher level. I think the architecture plus the relationships between the parameter also act as a compressing mechanism. You could most surely compress the information that is sent in a regular communication to a fairly small language. Despite that, it is still crazy to me. The combinatorics of the space of parameters learned is absolutely enormous, but the useful ones are probably a very small manifold or structure like that that lives somewhere in this high dimensional space.

I know that these models make things up very often because they are ultimately a language model, not a knowledge storing mechanism. Knowledge precision is not a metric that I believe is being optimized, so it seems to be an emergent property product of the complexity of the models.
